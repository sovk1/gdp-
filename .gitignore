# import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load preprocessed data into a Pandas DataFrame
data = pd.read_csv('preprocessed_data.csv')

# Select relevant features and target variable (GDP)
features = ['feature1', 'feature2', 'feature3', ...]
target = 'GDP'
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a gradient boosting machine model on the training data
model = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model's performance on the testing data
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error: %.2f" % mse)
print("R-squared: %.2f" % r2)

# Use the trained model to make predictions on new data
new_data = pd.read_csv('new_data.csv')
new_X = new_data[features]
new_predictions = model.predict(new_X)


##^^linear regression 
Above is simple linear regression
Below is gradient boost method. 

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load preprocessed data into a Pandas DataFrame
data = pd.read_csv('preprocessed_data.csv')

# Select relevant features and target variable (GDP)
features = ['feature1', 'feature2', 'feature3', ...]
target = 'GDP'
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a gradient boosting machine model on the training data
model = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model's performance on the testing data
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error: %.2f" % mse)
print("R-squared: %.2f" % r2)

# Use the trained model to make predictions on new data
new_data = pd.read_csv('new_data.csv')
new_X = new_data[features]
new_predictions = model.predict(new_X)

#^^
GradientBoostingRegressor class from scikit-learn to train a GBM model on the training data. The loss parameter is set to 'ls' for least squares regression, and the learning_rate parameter controls the contribution of each individual tree to the ensemble. The n_estimators parameter determines the number of trees in the model, while the max_depth parameter controls the maximum depth of each tree. The model is evaluated on the testing data using mean squared error and R-squared, and can be used to make predictions on new data using the predict() method.

Other advanced machine learning methods that could be used for GDP prediction include random forests, support vector regression, neural networks, and deep learning models. The choice of model depends on the specific requirements of the problem and the nature of the data.
Below is Random forest and xgboost

Xgboost code is below 

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load preprocessed data into a Pandas DataFrame
data = pd.read_csv('preprocessed_data.csv')

# Select relevant features and target variable (GDP)
features = ['feature1', 'feature2', 'feature3', ...]
target = 'GDP'
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost model on the training data
model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=5, n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model's performance on the testing data
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error: %.2f" % mse)
print("R-squared: %.2f" % r2)

# Use the trained model to make predictions on new data
new_data = pd.read_csv('new_data.csv')
new_X = new_data[features]
new_predictions = model.predict(new_X)


Random forest is below 

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load preprocessed data into a Pandas DataFrame
data = pd.read_csv('preprocessed_data.csv')

# Select relevant features and target variable (GDP)
features = ['feature1', 'feature2', 'feature3', ...]
target = 'GDP'
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest model on the training data
model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model's performance on the testing data
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error: %.2f" % mse)
print("R-squared: %.2f" % r2)

# Use the trained model to make predictions on new data
new_data = pd.read_csv('new_data.csv')
new_X = new_data[features]
new_predictions = model.predict(new_X)


Combining all the models (linear/xgboost/random forest)

import pandas as pd
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load preprocessed data into a Pandas DataFrame
data = pd.read_csv('preprocessed_data.csv')

# Select relevant features and target variable (GDP)
features = ['feature1', 'feature2', 'feature3', ...]
target = 'GDP'
X = data[features]
y = data[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model on the training data
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Train an XGBoost model on the training data
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=5, n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

# Train a random forest model on the training data
rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rf_model.fit(X_train, y_train)

# Combine the predictions from all three models using simple averaging
y_pred = (lr_model.predict(X_test) + xgb_model.predict(X_test) + rf_model.predict(X_test)) / 3

# Evaluate the combined model's performance on the testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error: %.2f" % mse)
print("R-squared: %.2f" % r2)

# Use the trained models to make predictions on new data and combine their predictions
new_data = pd.read_csv('new_data.csv')
new_X = new_data[features]
new_predictions = (lr_model.predict(new_X) + xgb_model.predict(new_X) + rf_model.predict(new_X)) / 3


